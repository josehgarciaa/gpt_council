{
    "module": {
        "authentication": {
            "files": {
                "auth_service": "\"\"\"\nford begin_TODO\n- Rename 'correct_login' to 'is_authenticated' to clearly indicate it is a boolean flag.\n- Consider renaming 'client' to 'openai_client' for better specificity.\n- Optionally, rename 'login' to 'authenticate' to better convey its purpose.\n- Ensure that print statements in production code are replaced by proper logging.\nend_todo\n\nModule: Authentication Service\nThis module provides an authentication service specialized for OpenAI.\nIt retrieves API keys from environment variables (via a .env file) or directly via parameters,\nand returns a fully authenticated OpenAI client.\n\"\"\"\n\nfrom authentication import SessionManager\nfrom typing import Optional\nfrom openai import OpenAI\n\n\nclass AuthenticationService:\n    \"\"\"\n    AuthenticationService provides functionality to authenticate and manage a session for OpenAI.\n\n    This class leverages a SessionManager to handle API key management. It can accept an API key directly,\n    or load it from a .env file if none is provided. Upon successful validation of the API key,\n    an OpenAI client is created and stored for further operations.\n\n    Attributes:\n        session_manager (SessionManager): Manages the API key, including setting, loading, and clearing it.\n        client (Optional[OpenAI]): Holds the authenticated OpenAI client once login is successful.\n        correct_login (bool): Flag indicating whether authentication was successful.\n\n    Methods:\n        login(api_key: Optional[str]): Authenticates by setting the API key and initializing the OpenAI client.\n        get_client(): Returns the authenticated OpenAI client if login was successful.\n        logout(): Logs out by clearing the session.\n        is_logged_in(): Checks if the current session is authenticated.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize the AuthenticationService instance.\n\n        Initializes the SessionManager, and sets up the client and authentication flag.\n        \"\"\"\n        self.session_manager = SessionManager()\n        self.client = None\n        self.correct_login = False\n\n    def login(self, api_key: Optional[str] = None):\n        \"\"\"\n        Authenticate the user and create an OpenAI client.\n\n        If an API key is provided, it is set in the session manager.\n        Otherwise, the API key is loaded from a .env file.\n        The method validates the API key format via the session manager and, upon success,\n        creates and stores an OpenAI client.\n\n        Args:\n            api_key (Optional[str]): The API key to authenticate with. If None, the API key will be loaded from the .env file.\n\n        Raises:\n            ValueError: If the API key format is invalid (i.e., authentication fails).\n        \"\"\"\n        if api_key is not None:\n            self.session_manager.set_api_key(api_key)\n        else:\n            self.session_manager.load_dotenv()\n\n        if not self.session_manager.is_authenticated:\n            raise ValueError(\"Authentication failed: Invalid API key format\")\n        print(\"Authentication successful.\")\n\n        if self.client is None:\n            self.client = OpenAI(api_key=self.session_manager.api_key)\n        self.correct_login = True\n\n    def get_client(self):\n        \"\"\"\n        Retrieve the authenticated OpenAI client.\n\n        Returns:\n            OpenAI: The authenticated OpenAI client if the login was successful.\n            None: If the authentication has not been performed or failed.\n        \"\"\"\n        if self.correct_login:\n            return self.client\n        else:\n            print(\"The Authenticator was not able to login or it was not logged\")\n            return None\n\n    def logout(self):\n        \"\"\"\n        Log out the user by clearing the session and resetting the authentication flag.\n\n        This method clears the stored API key and client information, effectively logging out the user.\n        \"\"\"\n        self.session_manager.clear_session()\n        self.correct_login = False\n        print(\"Logout successful.\")\n\n    def is_logged_in(self):\n        \"\"\"\n        Check if the user is currently authenticated.\n\n        Returns:\n            bool: True if the session is authenticated, False otherwise.\n        \"\"\"\n        return self.session_manager.is_authenticated\n",
                "session_manager": "\"\"\"\nford begin_TODO\n- Rename __instance to _instance to follow the common convention for internal variables.\n- Consider adding a note in the class docstring that this is a Singleton implementation.\n- Document the __new__ method to explain its role in the singleton pattern.\n- Use logging instead of print statements (if applicable in the context) for better production practices.\nend_todo\n\nModule: Session Manager\nThis module provides the SessionManager class which is responsible for reading,\nvalidating, and processing an API key before authenticating with the server.\nThe SessionManager is implemented as a singleton to ensure a single shared session state.\n\"\"\"\n\nimport os\nfrom dotenv import load_dotenv\n\n\nclass SessionManager:\n    \"\"\"\n    SessionManager handles the retrieval and validation of an API key for authentication.\n\n    This class reads the API key from a provided value or from a .env file,\n    validates its format, and manages the session state (authenticated or not).\n    It is implemented as a singleton, ensuring that only one instance exists\n    during the application's lifecycle.\n\n    Attributes:\n        api_key (str or None): The API key used for authentication.\n        is_authenticated (bool): Indicates whether the API key is valid.\n    \"\"\"\n\n    __instance = None  # Consider renaming to _instance for internal convention\n\n    def __new__(cls):\n        \"\"\"\n        Create and return a singleton instance of SessionManager.\n\n        If an instance already exists, this method returns it;\n        otherwise, it creates a new instance and initializes the attributes.\n        \"\"\"\n        if SessionManager.__instance is None:\n            SessionManager.__instance = object.__new__(cls)\n            SessionManager.__instance.api_key = None\n            SessionManager.__instance.is_authenticated = False\n        return SessionManager.__instance\n\n    def set_api_key(self, api_key: str):\n        \"\"\"\n        Set the API key and update the authentication status.\n\n        Args:\n            api_key (str): The API key to be set and validated.\n        \"\"\"\n        self.api_key = api_key\n        self.is_authenticated = self.validate_api_key(api_key)\n\n    def load_dotenv(self) -> None:\n        \"\"\"\n        Load the API key from a .env file and update the authentication status.\n\n        This method loads environment variables from the .env file,\n        retrieves the API key using the key \"API_KEY\", and validates it.\n        \"\"\"\n        load_dotenv()\n        self.api_key = os.getenv(\"API_KEY\")\n        self.is_authenticated = self.validate_api_key(self.api_key)\n\n    def validate_api_key(self, api_key: str) -> bool:\n        \"\"\"\n        Validate the format of the provided API key.\n\n        This method checks if the API key starts with the expected prefix (\"sk-\")\n        and meets a minimum length requirement.\n\n        Args:\n            api_key (str): The API key to validate.\n\n        Returns:\n            bool: True if the API key is valid, False otherwise.\n        \"\"\"\n        # Add actual validation logic, possibly checking on OpenAI's side.\n        # For demonstration, we check basic provisions:\n        return api_key.startswith(\"sk-\") and len(api_key) > 20\n\n    def clear_session(self):\n        \"\"\"\n        Clear the current session by resetting the API key and authentication status.\n\n        This method sets the API key to None and marks the session as not authenticated.\n        \"\"\"\n        self.api_key = None\n        self.is_authenticated = False\n",
                "__init__": "\"\"\"\nford begin_TODO\n- Ensure consistency in naming conventions across all modules in the package.\n- Optionally, include a changelog or author info for future reference.\nend_todo\n\nModule: authentication\nThis module aggregates the core components for managing API key sessions and authentication.\nIt exposes the SessionManager for handling API key retrieval and validation, and the\nAuthenticationService for authenticating users and obtaining a fully initialized client.\nThis setup ensures flexibility and clarity in how the authentication processes are configured\nand accessed across the package.\n\"\"\"\n\n__version__ = \"1.0.0\"\n\nfrom .session_manager import SessionManager\nfrom .auth_service import AuthenticationService\n"
            }
        },
        "chat_manager": {
            "files": {
                "api_response": "\"\"\"\nford begin_TODO\n- Consider renaming class 'APIResponse' to 'ApiResponse' for consistency.\n- Rename attribute 'call_api_value' to 'should_call_api' for clarity.\n- Rename attribute 'call_tool_value' to 'requires_tool_call' for clarity.\n- Rename method 'readable' to 'get_readable_message' to be more descriptive.\n- Rename method 'handle' to 'handle_response' to clarify its purpose.\n- Add type hints for method parameters and return types.\nend_todo\n\nModule: chat_manager.api_response\nDescription:\n    This module handles raw API responses from the chat client.\n    It determines whether further API calls are needed or if specific\n    client actions should be executed. It also processes the raw API\n    response into internal structures such as plain text or a structured\n    API response.\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import List, Optional, Dict\nfrom openai.types.chat import ChatCompletionMessageParam\nimport json\n\nclass APIResponse():\n    \"\"\"\n    A class for processing API responses within the chat manager.\n\n    This class analyzes raw API responses to determine if additional API\n    calls or client actions (e.g., tool invocations) are required. It also\n    converts the raw API response into a human-readable format.\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"\n        Initialize an APIResponse instance with default state.\n\n        Attributes:\n            message: Holds the API message (initially None).\n            call_api_value (bool): Flag indicating whether an API call should be made.\n            processed_content: The processed, human-readable content.\n            raw_api_response: The raw response data from the API.\n            call_tool_value (bool): Flag indicating whether a tool call is required.\n        \"\"\"\n        self.message = None\n        self.call_api_value = True \n        self.processed_content = None\n        self.raw_api_response = None\n        self.call_tool_value = False \n\n    def call_api(self):\n        \"\"\"\n        Check if an API call should be performed based on the response state.\n\n        Returns:\n            bool: True if an API call is needed, otherwise False.\n        \"\"\"\n        return self.call_api_value\n\n    def get_api_message(self):\n        \"\"\"\n        Extract and return the API message from the raw API response.\n\n        Returns:\n            The API message from the first choice in the raw API response.\n        \"\"\"\n        return self.raw_api_response.choices[0].message       \n        \n\n    def readable(self):\n        \"\"\"\n        Convert the raw API response into a human-readable string.\n\n        The method concatenates the role and content from the API message.\n\n        Returns:\n            str: A formatted string representing the API message.\n        \"\"\"\n        message = self.raw_api_response.choices[0].message\n        self.processed_content = \"role: \" + message.role + \"\\n\" + message.content\n        return self.processed_content\n\n    def required_action(self):\n        \"\"\"\n        Determine if a client action (e.g., a tool call) is required.\n\n        Returns:\n            bool: True if a tool call is required, otherwise False.\n        \"\"\"\n        return self.call_tool_value\n             \n     \n    def handle(self, raw_api_response):\n        \"\"\"\n        Process the raw API response and update the internal state accordingly.\n\n        This method sets the raw API response, checks for tool call indications,\n        and updates flags to indicate whether further API calls or client actions\n        are needed.\n\n        Args:\n            raw_api_response: The raw API response data to be processed.\n\n        Returns:\n            Either the original raw API response (if tool calls are detected) or\n            the updated APIResponse instance.\n        \"\"\"\n        self.raw_api_response = raw_api_response\n        \n        if self.raw_api_response.choices[0].message.tool_calls is not None:\n            self.call_tool_value = True                \n            self.call_api_value = True\n            return raw_api_response                                    \n        \n        self.call_tool_value = False                \n        self.call_api_value = False\n        self.raw_api_response = raw_api_response        \n        return self\n",
                "chat_developer_message": "\"\"\"\nford begin_TODO\n- Rename class \"ChatUserMessage\" to \"ChatUserMessageHandler\" to clarify its responsibility.\n- Rename attribute \"api_compatible_message\" to \"api_message\" for brevity and clarity.\n- Rename attribute \"readable_content\" to \"plain_text_message\" to better indicate its purpose.\n- Rename method \"handle\" to \"process_message\" to better reflect the action performed.\n- Rename method \"readable\" to \"get_readable_message\" to be more descriptive.\nend_todo\n\nModule: chat_manager.chat_user_message\nDescription:\n    This module handles user messages before they are sent to the chat controller.\n    It preprocesses raw text messages into formats that are compatible with the API as well\n    as human-readable text for internal processing.\n\"\"\"\n\nfrom openai.types.chat import ChatCompletionDeveloperMessageParam \n\n\nclass ChatDeveloperMessage:\n    \"\"\"\n    Handles user messages before sending them to the chat controller.\n    Performs preprocessing of raw text messages to generate both an API-compatible\n    message and a human-readable message format.\n\n    Attributes:\n        api_compatible_message (ChatCompletionMessageParam): The processed message\n            formatted for the chat server's API.\n        readable_content (str): A human-readable version of the message.\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"\n        Initializes an empty ChatUserMessage instance with default values.\n        \"\"\"\n        self.api_compatible_message = None    \n        self.readable_content = None     \n\n    def handle(self, content: str) -> \"ChatDeveloperMessage\":\n        \"\"\"\n        Processes the input message content and stores it as an API-compatible message.\n\n        Args:\n            content (str): The raw text message from the user.\n\n        Returns:\n            ChatUserMessage: The instance with the processed API-compatible message.\n        \"\"\"\n        # More preprocessing can be added in future implementations.\n        self.api_compatible_message = ChatCompletionDeveloperMessageParam(\n            role=\"developer\", content=content\n        )\n\n        return self\n\n    def get_api_message(self) -> ChatCompletionDeveloperMessageParam:\n        \"\"\"\n        Retrieves the API-compatible message.\n\n        Returns:\n            ChatCompletionDeveloperMessageParam: The processed message formatted for the API.\n        \"\"\"\n        return self.api_compatible_message\n\n    def readable(self) -> str:\n        \"\"\"\n        Generates a human-readable version of the API-compatible message.\n\n        Returns:\n            str: A formatted string containing the role and content of the message.\n        \"\"\"\n        message = self.api_compatible_message\n        self.readable_content = \"role:\" + message.get(\"role\") + \"\\n\" + message.get(\"content\")\n        return self.readable_content\n",
                "chat_user_message": "\"\"\"\nford begin_TODO\n- Rename class \"ChatUserMessage\" to \"ChatUserMessageHandler\" to clarify its responsibility.\n- Rename attribute \"api_compatible_message\" to \"api_message\" for brevity and clarity.\n- Rename attribute \"readable_content\" to \"plain_text_message\" to better indicate its purpose.\n- Rename method \"handle\" to \"process_message\" to better reflect the action performed.\n- Rename method \"readable\" to \"get_readable_message\" to be more descriptive.\nend_todo\n\nModule: chat_manager.chat_user_message\nDescription:\n    This module handles user messages before they are sent to the chat controller.\n    It preprocesses raw text messages into formats that are compatible with the API as well\n    as human-readable text for internal processing.\n\"\"\"\n\nfrom openai.types.chat import ChatCompletionUserMessageParam, ChatCompletionMessageParam\n\n\nclass ChatUserMessage:\n    \"\"\"\n    Handles user messages before sending them to the chat controller.\n    Performs preprocessing of raw text messages to generate both an API-compatible\n    message and a human-readable message format.\n\n    Attributes:\n        api_compatible_message (ChatCompletionMessageParam): The processed message\n            formatted for the chat server's API.\n        readable_content (str): A human-readable version of the message.\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"\n        Initializes an empty ChatUserMessage instance with default values.\n        \"\"\"\n        self.api_compatible_message = None    \n        self.readable_content = None     \n\n    def handle(self, content: str) -> \"ChatUserMessage\":\n        \"\"\"\n        Processes the input message content and stores it as an API-compatible message.\n\n        Args:\n            content (str): The raw text message from the user.\n\n        Returns:\n            ChatUserMessage: The instance with the processed API-compatible message.\n        \"\"\"\n        # More preprocessing can be added in future implementations.\n        self.api_compatible_message = ChatCompletionUserMessageParam(\n            role=\"user\", content=content\n        )\n        return self\n\n    def get_api_message(self) -> ChatCompletionMessageParam:\n        \"\"\"\n        Retrieves the API-compatible message.\n\n        Returns:\n            ChatCompletionMessageParam: The processed message formatted for the API.\n        \"\"\"\n        return self.api_compatible_message\n\n    def readable(self) -> str:\n        \"\"\"\n        Generates a human-readable version of the API-compatible message.\n\n        Returns:\n            str: A formatted string containing the role and content of the message.\n        \"\"\"\n        message = self.api_compatible_message\n        self.readable_content = \"role:\" + message.get(\"role\") + \"\\n\" + message.get(\"content\")\n        return self.readable_content\n",
                "client_action": "\"\"\"\nford begin_TODO\n- Consider renaming class 'ClientAction' to 'ClientActionHandler' for clarity.\n- Rename method 'get_api_message' to 'get_api_messages' to indicate it returns a list.\n- Rename method 'required' to 'is_action_required' for improved readability.\n- Consider adding type hints for method parameters and return types.\nend_todo\n\nModule: chat_manager.client_action\nDescription:\n    This module handles client actions required by the chat manager.\n    It processes API responses to determine if any client actions are needed,\n    and coordinates the creation of action messages using model tool invocations.\n\"\"\"\n\nfrom chat_manager import APIResponse\nimport json\n\nclass ClientAction():\n    \"\"\"\n    Handles client actions as part of the chat management system.\n\n    This class is responsible for processing API responses to extract\n    tool calls, executing those calls via the model's tools, and maintaining\n    a list of API messages to be sent back to the client.\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"\n        Initializes a new instance of ClientAction with an empty list of API messages.\n        \"\"\"\n        self.api_messages = []\n\n    def get_api_message(self):\n        \"\"\"\n        Retrieve the list of API messages that have been generated.\n\n        Returns:\n            list: The list of API messages.\n        \"\"\"\n        return self.api_messages\n                     \n    def required(self, api_response):\n        \"\"\"\n        Check if a client action is required based on the API response.\n\n        Args:\n            api_response (APIResponse): The APIResponse object containing the response data.\n\n        Returns:\n            bool: True if a client action is required, otherwise False.\n        \"\"\"\n        return api_response.required_action()\n    \n    def execute(self, model, api_response):\n        \"\"\"\n        Execute client actions based on tool calls found in the API response.\n\n        For each tool call in the API response, this method retrieves the\n        corresponding tool from the model, calls it with the provided arguments,\n        and constructs an API message which is then appended to the internal list.\n\n        Args:\n            model: The model object containing the tools list.\n            api_response: The APIResponse object with raw response data.\n\n        Returns:\n            bool: True after processing the tool calls.\n        \"\"\"\n        for tool_call in api_response.raw_api_response.choices[0].message.tool_calls:\n            api_message = {\"role\": \"tool\", \"tool_call_id\": tool_call.id, \"content\": \"\"}\n            function_name = tool_call.function.name\n            function_args = json.loads(tool_call.function.arguments)\n            api_message[\"content\"] = str(model.tools_list.get_tool_by_name(function_name).call_function(function_args))\n            self.api_messages.append(api_message)\n        return True \n",
                "handler": "\"\"\"\nford begin_TODO\n- Consider renaming class \"ChatManager\" to \"ChatManagerFacade\" for clearer intent.\n- In the __init__ docstring, update parameter names to match the actual parameter (\"authenticator\" instead of \"auth\").\n- Improve the return value documentation in the send_message method (e.g., \"True if the message was processed successfully, False otherwise\").\n- Replace print statements with proper logging for production-level error reporting.\n- Add type hints for the \"chatbot\" parameter in get_response for better clarity.\n- Clarify exception handling in get_response (avoid using raise print(...)).\nend_todo\n\nModule: chat_manager.handler\nDescription:\n    The ChatManager module serves as the facade for handling chat interactions.\n    It is responsible for sending user messages to the client, capturing responses,\n    and delegating the processing of these responses to appropriate internal structures\n    and classes (such as ChatUserMessage, APIResponse, ChatHistory, and ClientAction).\n\nClasses:\n    ChatManager:\n        A facade that orchestrates the processing of user messages and responses.\n        It integrates authentication services, maintains conversation history,\n        and coordinates client actions based on API responses.\n\nUsage:\n    The ChatManager is initialized with an authentication service and provides methods\n    to send messages and retrieve responses from the chat client. It also supports\n    clearing the conversation history if needed.\n\"\"\"\n\nfrom authentication import AuthenticationService\nfrom chat_manager import ChatUserMessage, ChatDeveloperMessage, APIResponse, ChatHistory, ClientAction\n\nclass ChatManager:\n    \"\"\"\n    A facade for managing chat interactions, authentication, and monitoring.\n\n    This class does not maintain stateful ModelConfig or Model objects; instead,\n    these are expected to be passed dynamically with each request. This design\n    enhances flexibility and avoids unnecessary state management.\n    \"\"\"\n\n    def __init__(self, authenticator: AuthenticationService) -> None:\n        \"\"\"\n        Initializes the ChatManager with an authentication service and sets up\n        the conversation history.\n\n        Args:\n            authenticator (AuthenticationService): Handles authentication for API requests.\n        \"\"\"\n        self.auth = authenticator\n        # self.monitor = monitor  # Monitoring service can be added if needed.\n        self.chat_history = ChatHistory()  # Structured message storage.\n        self.developer_message = \"\"\n\n\n    def send_developer(self, user_text: str) -> bool:\n        \"\"\"\n        Processes the user's message, stores it internally, and returns a status.\n\n        The method preprocesses the raw user text into an API-compatible message\n        using ChatUserMessage and appends it to the chat history.\n\n        Args:\n            user_text (str): The text input from the user.\n\n        Returns:\n            bool: True if the message was processed and stored successfully;\n                  False otherwise.\n        \"\"\"\n        try:\n            developer_message = ChatDeveloperMessage().handle(user_text)\n            self.chat_history.append_message(developer_message)\n            return True\n        except Exception:\n            raise\"The user message could not be processed\"\n\n\n\n    def send_message(self, user_text: str) -> bool:\n        \"\"\"\n        Processes the user's message, stores it internally, and returns a status.\n\n        The method preprocesses the raw user text into an API-compatible message\n        using ChatUserMessage and appends it to the chat history.\n\n        Args:\n            user_text (str): The text input from the user.\n\n        Returns:\n            bool: True if the message was processed and stored successfully;\n                  False otherwise.\n        \"\"\"\n        try:\n            user_message = ChatUserMessage().handle(user_text)\n            self.chat_history.append_message(user_message)\n            return True\n        except Exception:\n            print(\"The user message could not be processed\")\n            return False\n\n    def get_response(self, chatbot) -> APIResponse:\n        \"\"\"\n        Processes an incoming chat message and determines an appropriate response.\n\n        This method expects a tuple containing a model configuration and a model.\n        It then continuously checks if further responses are needed by calling\n        the API via the authentication service's client. The method also handles\n        client actions as required and updates the conversation history accordingly.\n\n        Args:\n            chatbot: A tuple (model_config, model) where:\n                - model_config: Contains configuration parameters for the API call.\n                - model: Contains model-specific attributes, such as model_type and tools_list.\n\n        Returns:\n            APIResponse: A readable representation of the final API response.\n        \"\"\"\n        model_config, model = chatbot          \n        api_response = APIResponse() \n        print(\"sending a message with model\", model.model_type)\n        # Check if more responses are necessary.\n        while api_response.call_api():\n            # Determine the actions to take based on the API response.\n            raw_api_response = self.auth.get_client().chat.completions.create(\n                model=model.model_type,\n                messages=self.chat_history.messages(),\n                tools=model.tools_list.get_all_schemas(),\n                **(model_config.get_params())\n            )\n            # Handle the response.\n            try:\n                api_response.handle(raw_api_response)\n            except Exception:\n                raise print(\"Problem handling API response\")\n            self.chat_history.append_message(api_response)\n\n            # Determine if the API required a client action.\n            client_action = ClientAction()\n            if client_action.required(api_response):\n                try:\n                    client_action.execute(model, api_response)\n                except Exception:\n                    print(\"Problem executing client action\")\n                    raise\n                self.chat_history.append_message(client_action)\n\n            print(self.chat_history.messages)\n\n        return api_response.readable()\n\n    def clear_history(self) -> None:\n        \"\"\"\n        Clears the developer message and (optionally) the conversation history.\n\n        Note:\n            The method to clear chat_history messages is currently commented out.\n        \"\"\"\n        self.chat_history.clear_messages()\n",
                "history_manager": "\"\"\"\nford begin_TODO\n- Rename the \"history\" attribute to \"messages_history\" or simply \"messages\" for clarity.\n- Consider renaming \"append_message\" to \"add_message\" for consistency with common naming conventions.\n- Clarify the expected types in the type hint for the \"message\" parameter in append_message (e.g., include APIResponse and ClientAction as well).\n- Ensure that the exception message in append_message clearly indicates which type is expected.\nend_todo\n\nModule: chat_manager.history_manager\nDescription:\n    This module handles the chat history, storing all chat messages in a\n    format that can be easily processed by other classes and functions in\n    the application. The ChatHistory class provides methods to append new\n    messages and to retrieve the entire conversation history in an API-\n    compatible format.\n    \nClasses:\n    ChatHistory:\n        A data class that maintains a list of messages. It provides methods\n        for appending individual or multiple messages and for retrieving the\n        complete chat history.\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom typing import List, Union, Iterable\nfrom chat_manager import APIResponse, ChatUserMessage, ClientAction, ChatDeveloperMessage\n\n\n@dataclass\nclass ChatHistory:\n    \"\"\"\n    Stores the history of chat messages, allowing messages to be appended\n    and retrieved in an API-compatible format.\n    \n    Attributes:\n        history (List[dict]): A list that stores messages as dictionaries.\n    \"\"\"\n    history: List[dict] = field(default_factory=list)\n\n    def append_message(self, message: Union[ChatUserMessage, APIResponse, ClientAction]) -> None:\n        \"\"\"\n        Appends a message or a collection of messages to the chat history.\n        \n        Depending on the type of the message, this method retrieves the API-\n        compatible message(s) and adds them to the history.\n        \n        Args:\n            message (Union[ChatUserMessage, APIResponse, ClientAction]): A single\n                message instance or an object providing the 'get_api_message()' method.\n        \n        Raises:\n            AttributeError: If the provided object does not have a 'get_api_message()'\n                method.\n        \"\"\"\n        if isinstance(message, (ChatUserMessage,ChatDeveloperMessage, APIResponse)):\n            # Single message case.\n            self.history.append(message.get_api_message())\n        elif isinstance(message, ClientAction):\n            # Handle multiple messages.\n            for api_message in message.get_api_message():\n                self.history.append(api_message)\n        else:\n            raise AttributeError(f\"Object {message} does not have 'get_api_message()' method.\")\n\n    def messages(self) -> List[dict]:\n        \"\"\"\n        Retrieves the entire chat history.\n        \n        Returns:\n            List[dict]: A list of messages formatted as API-compatible dictionaries.\n        \"\"\"\n        return self.history\n    \n    def clear_messages(self):\n        print(\"deleting history\")\n        self.history = []\n",
                "__init__": "\"\"\"\nford begin_TODO\n- Avoid using wildcard imports to prevent namespace pollution; consider explicit imports.\n- Ensure consistency in naming conventions across submodules (e.g., consider renaming ChatUserMessage to ChatUserMessageHandler).\n- Consider renaming \"handler\" to \"chat_manager_facade\" for better clarity.\nend_todo\n\nModule: chat_manager\nVersion: 1.0.0\n\nDescription:\n    The chat_manager module serves as the central component for managing chat interactions\n    between the client and the server. It aggregates various submodules responsible for processing\n    user messages, handling API responses, managing chat history, and executing client actions.\n\nSubmodules:\n    - chat_user_message: Processes and prepares user messages for the chat system.\n    - api_response: Handles raw API responses, determines necessary follow-up actions, and converts responses\n      into internal formats.\n    - client_action: Executes actions on the client side based on API responses and model tool calls.\n    - history_manager: Manages the storage and retrieval of the complete chat history.\n    - handler: Provides the ChatManager facade for orchestrating the overall chat interactions.\n\"\"\"\n\nfrom .chat_user_message import *\nfrom .chat_developer_message import *\nfrom .api_response import *\nfrom .client_action import ClientAction\nfrom .history_manager import ChatHistory\nfrom .handler import ChatManager\n"
            }
        },
        "helpers": {
            "files": {
                "utils": "from pathlib import Path\nfrom typing import Dict, Any, Union\n\n\nfrom pathlib import Path\nfrom typing import Optional\n\n\ndef safe_read_file(file_path: str) -> Optional[str]:\n    \"\"\"\n    Safely reads the content of the file at the given file path.\n\n    This function accepts the file path as a string, converts it to a Path object,\n    and attempts to read its content using UTF-8 encoding. If the file does not exist\n    or an error occurs during reading, it prints an error message and returns None.\n\n    Args:\n        file_path (str): The path to the file to be read.\n\n    Returns:\n        Optional[str]: The content of the file if successful, otherwise None.\n    \"\"\"\n    path = Path(file_path)\n    if not path.is_file():\n        print(f\"Error: The file {file_path} does not exist or is not a file.\")\n        return None\n\n    try:\n        return path.read_text(encoding=\"utf-8\")\n    except Exception as e:\n        print(f\"Error reading file {file_path}: {e}\")\n        return None\n\n\ndef safe_write_file(content: str, file_path: str) -> bool:\n    \"\"\"\n    Safely writes the given content to a file at the specified file path.\n\n    This function accepts the destination file path as a string, converts it to a\n    Path object, ensures that the directory exists (creating it if necessary), and\n    writes the content using UTF-8 encoding. It handles any exceptions by printing\n    an error message.\n\n    Args:\n        content (str): The content to write to the file.\n        file_path (str): The path to the file where the content should be written.\n\n    Returns:\n        bool: True if the file was written successfully, False otherwise.\n    \"\"\"\n    path = Path(file_path)\n    try:\n        # Ensure the parent directory exists\n        path.parent.mkdir(parents=True, exist_ok=True)\n        path.write_text(content, encoding=\"utf-8\")\n        return True\n    except Exception as e:\n        print(f\"Error writing to file {file_path}: {e}\")\n        return False\n\n\ndef read_project(root_dir: str | Path) -> Dict[str, Any]:\n    \"\"\"\n    Recursively reads a project's directory structure starting from `root_dir` and\n    returns a nested dictionary representing the structure.\n    \n    Python files (.py) in each directory are read and stored as a dictionary under\n    the \"files\" key, where each file name maps to its content.\n    \n    In each directory, if a \".crawler_ignore\" file exists, any files or directories\n    listed there (one per line, ignoring blank lines and comments starting with '#')\n    are skipped from the scan.\n    \n    Args:\n        root_dir (str or Path): The root directory of the project to scan.\n    \n    Returns:\n        Dict[str, Any]: A nested dictionary representing the project structure.\n    \"\"\"\n    # Convert to Path if necessary\n    if not isinstance(root_dir, Path):\n        root_dir = Path(root_dir)\n\n    project_dict = {}\n\n    # Build ignore set from .crawler_ignore if it exists\n    ignore_set = set()\n    ignore_file = root_dir / \".crawler_ignore\"\n    if ignore_file.is_file():\n        for line in ignore_file.read_text(encoding=\"utf-8\").splitlines():\n            line = line.strip()\n            if line and not line.startswith(\"#\"):\n                ignore_set.add(line)\n\n    # Read Python files in the current directory\n    files = {}\n    for item in root_dir.iterdir():\n        # Skip hidden items and those in the ignore list\n        if item.name.startswith('.') or item.name in ignore_set:\n            continue\n        if item.is_file() and item.suffix == \".py\":\n            try:\n                files[item.name] = item.read_text(encoding=\"utf-8\")\n            except Exception as e:\n                files[item.name] = f\"Error reading file: {e}\"\n    if files:\n        project_dict[\"files\"] = files\n\n    # Recursively process subdirectories\n    for item in root_dir.iterdir():\n        if item.name.startswith('.') or item.name in ignore_set:\n            continue\n        if item.is_dir():\n            sub_dict = read_project(item)\n            if sub_dict:\n                project_dict[item.name] = sub_dict\n\n    return project_dict\n\n\n\ndef write_project(project_dict: Dict[str, Any], dest_dir: Union[str, Path]) -> None:\n    \"\"\"\n    Recreates the project structure from `project_dict` into the destination directory\n    `dest_dir`. The dictionary format should match that returned by `read_project`.\n\n    Files (under the \"files\" key) are created with their corresponding content, and any\n    subdirectories are recursively created.\n\n    Args:\n        project_dict (Dict[str, Any]): A nested dictionary representing the project structure.\n        dest_dir (str or Path): The destination directory where the project should be recreated.\n\n    Raises:\n        Exception: If any file or directory cannot be created or written.\n    \"\"\"\n    # Ensure dest_dir is a Path object\n    if not isinstance(dest_dir, Path):\n        dest_dir = Path(dest_dir)\n\n    dest_dir.mkdir(parents=True, exist_ok=True)\n\n    # Write files in the current directory\n    files = project_dict.get(\"files\", {})\n    for file_name, content in files.items():\n        file_path = dest_dir / file_name\n        try:\n            file_path.write_text(content, encoding=\"utf-8\")\n        except Exception as e:\n            print(f\"Error writing file {file_path}: {e}\")\n\n    # Process subdirectories recursively\n    for key, value in project_dict.items():\n        if key == \"files\":\n            continue\n        sub_dir = dest_dir / key\n        try:\n            sub_dir.mkdir(parents=True, exist_ok=True)\n        except Exception as e:\n            print(f\"Error creating directory {sub_dir}: {e}\")\n            continue\n        if isinstance(value, dict):\n            write_project(value, sub_dir)\n            \ndef format_project_structure(project_content: dict) -> str:\n    \"\"\"\n    Formats a given project dictionary into a readable string representation.\n\n    Args:\n        project_content (dict): A dictionary representing the project structure.\n            Expected format:\n            {\n                \"module_name\": {\n                    \"files\": {\n                        \"filename\": \"file_content\"\n                    }\n                }\n            }\n\n    Returns:\n        str: A formatted string representing the project structure.\n    \"\"\"\n    project_map = \"\"\n    \n    for module_name, module_data in project_content.items():\n        project_map += f\"Module Name: {module_name}\\n\"\n        \n        if \"files\" in module_data:\n            for file_name, file_content in module_data[\"files\"].items():\n                project_map += f\"File Name: {file_name}\\n\"\n                project_map += file_content\n                project_map += \"\\n\"\n                \n    return project_map\n\n\n",
                "__init__": "from .utils import *"
            }
        },
        "models": {
            "files": {
                "model": "\"\"\"\nford begin_TODO\n- Rename references to \"administrator\" in docstrings to \"developer\" to match the attribute name.\n- Update the docstring for set_model_type to correctly describe its purpose.\n- Consider consolidating or clarifying the usage of \"tools\" vs. \"tools_list\" attributes.\n- Ensure parameter names in docstrings match those in method signatures.\nend_todo\n\nModule: models.model\nDescription: Defines a builder-style Model class following a director design pattern.\nThis module stores the configuration for a model including its inputs, outputs, and the\nparameters for generating outputs based on the selected model_type. The corresponding\nModelDirector (defined in model_director.py) uses this configuration to properly set up\nand control the model.\n\"\"\"\n\nfrom typing import Any, Dict, List, Optional\nfrom tools import ModelTool, ModelToolList\n\n\nclass Model:\n    \"\"\"\n    A builder-style class for configuring model parameters such as output types,\n    streaming options, and more. Each setter returns the current instance to\n    support fluent (chained) method calls.\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"\n        Initialize default values for all configurable fields.\n        These defaults can be overridden by calling the provided setter methods.\n        \"\"\"\n        self.developer: str = \"You are a useful assistant\"\n        self.model_type: str = \"gpt-4o-mini\"\n        self.modalities: Optional[List[str]] = [\"text\"]\n        self.audio: Optional[Dict[str, Any]] = None\n        self.response_format: Optional[Dict[str, Any]] = None\n        self.stream: bool = False\n        self.stream_options: Optional[Dict[str, Any]] = None\n        self.tools: Optional[List[Any]] = None  # Note: This attribute is currently unused.\n        self.parallel_tool_calls: bool = True\n        self.user: str = \"\"\n        self.tools_list: Optional[ModelToolList] = None\n        self.tools_schema = None\n\n    def set_developer_instruction(self, developer: str) -> \"Model\":\n        \"\"\"\n        Set the developer identifier for this model configuration.\n\n        Args:\n            developer (str): The developer or owner responsible for this model.\n\n        Returns:\n            Model: The current Model instance (for fluent chaining).\n        \"\"\"\n        self.developer = developer\n        return self\n\n    def set_model_type(self, model_type: str) -> \"Model\":\n        \"\"\"\n        Set the model type for this model configuration.\n\n        Args:\n            model_type (str): The type of the model (e.g., 'gpt-4o-mini').\n\n        Returns:\n            Model: The current Model instance (for fluent chaining).\n        \"\"\"\n        self.model_type = model_type\n        return self\n\n    def set_modalities(self, modalities: List[str]) -> \"Model\":\n        \"\"\"\n        Specify one or more output modalities the model should generate.\n        Typical values could be [\"text\"] or [\"text\", \"audio\"].\n\n        Args:\n            modalities (List[str]): A list of desired output modalities.\n\n        Returns:\n            Model: The current Model instance (for fluent chaining).\n        \"\"\"\n        self.modalities = modalities\n        return self\n\n    def set_audio(self, audio: Dict[str, Any]) -> \"Model\":\n        \"\"\"\n        Provide configuration for audio output parameters.\n\n        Args:\n            audio (Dict[str, Any]): A dictionary with audio-related configuration,\n                                    e.g., codec or audio format settings.\n\n        Returns:\n            Model: The current Model instance (for fluent chaining).\n        \"\"\"\n        self.audio = audio\n        return self\n\n    def set_response_format(self, response_format: Dict[str, Any]) -> \"Model\":\n        \"\"\"\n        Specify the desired output format (e.g., JSON schema).\n\n        Args:\n            response_format (Dict[str, Any]): A dictionary defining the output format.\n\n        Returns:\n            Model: The current Model instance (for fluent chaining).\n        \"\"\"\n        self.response_format = response_format\n        return self\n\n    def set_stream(self, stream: bool) -> \"Model\":\n        \"\"\"\n        Toggle streaming mode for partial token outputs in real time.\n\n        Args:\n            stream (bool): True to enable streaming, False otherwise.\n\n        Returns:\n            Model: The current Model instance (for fluent chaining).\n        \"\"\"\n        self.stream = stream\n        return self\n\n    def set_stream_options(self, stream_options: Dict[str, Any]) -> \"Model\":\n        \"\"\"\n        Configure additional streaming options.\n\n        Args:\n            stream_options (Dict[str, Any]): A dictionary with stream-related parameters.\n\n        Returns:\n            Model: The current Model instance (for fluent chaining).\n        \"\"\"\n        self.stream_options = stream_options\n        return self\n\n    def set_tool(self, external_tool: object) -> \"Model\":\n        \"\"\"\n        Add a tool to the model's list of callable tools.\n\n        Args:\n            external_tool (object): A tool (function or callable) to be added.\n\n        Returns:\n            Model: The current Model instance (for fluent chaining).\n        \"\"\"\n        if self.tools_list is None:\n            self.tools_list = ModelToolList()\n\n        self.tools_list.add_tool(ModelTool().set_function(external_tool))\n        return self\n\n    def set_tools(self, external_tools: List[object]) -> \"Model\":\n        \"\"\"\n        Add a list of tools to the model's list of callable tools.\n\n        Args:\n            external_tools (List[object]): A list of tools (functions or callables) to be added.\n\n        Returns:\n            Model: The current Model instance (for fluent chaining).\n        \"\"\"\n        if self.tools_list is None:\n            self.tools_list = ModelToolList()\n\n        for external_tool in external_tools:\n            self.set_tool(external_tool)\n        return self\n\n    def enable_parallel_tool_calls(self, enable: bool) -> \"Model\":\n        \"\"\"\n        Enable or disable parallel calls to multiple tools.\n\n        Args:\n            enable (bool): True to enable parallel tool calls, False to disable.\n\n        Returns:\n            Model: The current Model instance (for fluent chaining).\n        \"\"\"\n        self.parallel_tool_calls = enable\n        return self\n\n    def set_user(self, user: str) -> \"Model\":\n        \"\"\"\n        Assign a unique user identifier, useful for tracking or analytics.\n\n        Args:\n            user (str): A string identifying the end-user.\n\n        Returns:\n            Model: The current Model instance (for fluent chaining).\n        \"\"\"\n        self.user = user\n        return self\n\n    def build(self) -> \"Model\":\n        \"\"\"\n        Finalize and return the fully configured Model object.\n\n        This method can include validation or post-processing steps if needed.\n\n        Returns:\n            Model: The fully configured Model instance.\n        \"\"\"\n        # Example validation (optional):\n        # if self.stream and not self.stream_options:\n        #     raise ValueError(\"Stream options must be set if streaming is enabled.\")\n        return self\n\n    def __repr__(self) -> str:\n        \"\"\"\n        Provide a concise string representation of the model configuration.\n\n        Returns:\n            str: A string summarizing key configuration parameters.\n        \"\"\"\n        return (\n            f\"<Model developer={self.developer}, \"\n            f\"modalities={self.modalities}, stream={self.stream}, ...>\"\n        )\n",
                "model_config": "\"\"\"\nford begin_TODO\n- Suggest renaming the class from \"Config\" to \"ModelConfig\" to clarify its purpose.\n- Consider updating the return type annotations in the setter methods to \"ModelConfig\" if the class is renamed.\n- In the __repr__ method, the attributes 'self.model' and 'self.messages' are referenced but not defined; review and update accordingly.\n- Optionally, add more detailed parameter type descriptions in the docstrings where beneficial.\nend_todo\n\nThis module defines a builder-style class for configuring chat completion\nparameters. The class provides fluent setter methods, each returning self,\nmaking it simple to chain multiple configurations in one statement.\n\nPEP 8 compliance note:\n- Line length is kept at or below 79 characters (per PEP 8 recommendations).\n- Docstrings include brief descriptions of each method, referencing the\n  relevant API parameters as described in the specification.\n\"\"\"\n\nfrom typing import Any, Dict, List, Optional, Union\n\n\nclass Config:\n    \"\"\"\n    A builder-style class for configuring chat completion parameters.\n\n    Each setter returns the current instance (self) to allow method chaining.\n    The final configuration can be used directly or validated further if needed.\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"\n        Initializes default values for all parameters. These defaults reflect\n        either the documented API defaults or typical starting points.\n        \"\"\"\n        # Optional (with documented defaults or None)\n        self.store: bool = None\n        self.reasoning_effort: str = None\n        self.metadata: Dict[str, str] = None\n        self.frequency_penalty: float = None\n        self.logit_bias: Dict[str, float] = None\n        self.logprobs: bool = None\n        self.top_logprobs: int = None\n        self.max_completion_tokens: int = None\n        self.n: int = None\n        self.prediction: Dict[str, Any] = None\n        self.presence_penalty: float = None\n        self.seed: int = None\n        self.service_tier: str = None\n        self.stop: Union[str, List[str], None] = None\n        self.temperature: float = None\n        self.top_p: float = None\n        \n    \n    def set_store(self, store: bool) -> \"ModelConfig\":\n        \"\"\"\n        Specify whether to store the output for use in model distillation.\n\n        :param store: Optional (defaults to False).\n        :return: The current ModelConfig instance.\n        \"\"\"\n        self.store = store\n        return self\n\n    def set_reasoning_effort(self, effort: str) -> \"ModelConfig\":\n        \"\"\"\n        Constrain the reasoning effort for compatible models.\n\n        :param effort: Optional (defaults to \"medium\"). Valid values are\n                       \"low\", \"medium\", or \"high\" for o1/o3-mini models only.\n        :return: The current ModelConfig instance.\n        \"\"\"\n        self.reasoning_effort = effort\n        return self\n\n    def set_metadata(self, metadata: Dict[str, str]) -> \"ModelConfig\":\n        \"\"\"\n        Attach additional metadata as key-value pairs.\n\n        :param metadata: A dictionary with up to 16 key-value pairs,\n                         keys up to 64 chars, values up to 512 chars.\n        :return: The current ModelConfig instance.\n        \"\"\"\n        self.metadata = metadata\n        return self\n\n    def set_frequency_penalty(self, penalty: float) -> \"ModelConfig\":\n        \"\"\"\n        Adjust repetition penalty based on token frequency.\n\n        :param penalty: A float between -2.0 and 2.0. Positive values penalize\n                        repeating tokens, reducing verbatim repetition.\n        :return: The current ModelConfig instance.\n        \"\"\"\n        self.frequency_penalty = penalty\n        return self\n\n    def set_logit_bias(self, bias: Dict[str, float]) -> \"ModelConfig\":\n        \"\"\"\n        Modify the likelihood of specified tokens appearing.\n\n        :param bias: A dictionary mapping token IDs to bias values\n                     (-100 to 100). Large positive/negative values can ban\n                     or force tokens.\n        :return: The current ModelConfig instance.\n        \"\"\"\n        self.logit_bias = bias\n        return self\n\n    def set_logprobs(self, log_probs: bool) -> \"ModelConfig\":\n        \"\"\"\n        Specify whether to return log probabilities of output tokens.\n\n        :param log_probs: Optional (defaults to False). If True, each output\n                          token's log probability will be included.\n        :return: The current ModelConfig instance.\n        \"\"\"\n        self.logprobs = log_probs\n        return self\n\n    def set_top_logprobs(self, top_logprobs: int) -> \"ModelConfig\":\n        \"\"\"\n        Specify how many top tokens to return in log probabilities.\n\n        :param top_logprobs: Integer (0 to 20). Must have logprobs=True\n                             for this to take effect.\n        :return: The current ModelConfig instance.\n        \"\"\"\n        self.top_logprobs = top_logprobs\n        return self\n\n    def set_max_completion_tokens(self, max_tokens: int) -> \"ModelConfig\":\n        \"\"\"\n        Set the maximum number of tokens to generate.\n\n        :param max_tokens: Upper bound on tokens for both visible output and\n                           internal reasoning. This is preferred over\n                           the deprecated max_tokens parameter.\n        :return: The current ModelConfig instance.\n        \"\"\"\n        self.max_completion_tokens = max_tokens\n        return self\n\n    def set_n(self, n: int) -> \"ModelConfig\":\n        \"\"\"\n        Set how many chat completion choices to generate.\n\n        :param n: Defaults to 1. Increasing this will generate multiple\n                  responses but will cost more tokens.\n        :return: The current ModelConfig instance.\n        \"\"\"\n        self.n = n\n        return self\n\n    def set_presence_penalty(self, penalty: float) -> \"ModelConfig\":\n        \"\"\"\n        Penalize tokens based on whether they appear in the text so far.\n\n        :param penalty: A float between -2.0 and 2.0. Positive values encourage\n                        the model to avoid repetition.\n        :return: The current ModelConfig instance.\n        \"\"\"\n        self.presence_penalty = penalty\n        return self\n\n    def set_seed(self, seed: int) -> \"ModelConfig\":\n        \"\"\"\n        Set a seed for best-effort deterministic sampling.\n\n        :param seed: An integer seed. Repeated requests with identical\n                     parameters and seed may produce the same output.\n        :return: The current ModelConfig instance.\n        \"\"\"\n        self.seed = seed\n        return self\n\n    def set_service_tier(self, tier: str) -> \"ModelConfig\":\n        \"\"\"\n        Specify the service tier for processing the request.\n\n        :param tier: Defaults to \"auto\". If set to \"auto\" and the project\n                     is scale-tier enabled, scale-tier credits are used.\n        :return: The current ModelConfig instance.\n        \"\"\"\n        self.service_tier = tier\n        return self\n\n    def set_stop(self, stop: Union[str, List[str]]) -> \"ModelConfig\":\n        \"\"\"\n        Define one or more sequences that will stop token generation.\n\n        :param stop: A string or a list of up to 4 strings to halt generation.\n        :return: The current ModelConfig instance.\n        \"\"\"\n        self.stop = stop\n        return self\n\n    def set_temperature(self, temperature: float) -> \"ModelConfig\":\n        \"\"\"\n        Control randomness in sampling between 0 and 2.\n\n        :param temperature: Defaults to 1.0. Higher values increase\n                            randomness, lower values make output more\n                            deterministic.\n        :return: The current ModelConfig instance.\n        \"\"\"\n        self.temperature = temperature\n        return self\n\n    def set_top_p(self, top_p: float) -> \"ModelConfig\":\n        \"\"\"\n        Use nucleus sampling, considering tokens up to top_p probability.\n\n        :param top_p: Defaults to 1.0. If 0.1, only the tokens comprising\n                      the top 10% probability mass are considered.\n        :return: The current ModelConfig instance.\n        \"\"\"\n        self.top_p = top_p\n        return self\n\n    def get_params(self) -> Dict[str, Any]:\n        \"\"\"\n        Gather all configuration parameters with non-None values and return them\n        as a dictionary.\n\n        This method iterates through the instance's attributes and collects only\n        those whose values are not None. This is useful when you need to pass a\n        configuration dictionary to a function that accepts various parameters,\n        allowing you to use dictionary unpacking (**).\n\n        Returns:\n            Dict[str, Any]: A dictionary of parameter names and their values.\n        \"\"\"\n        return {key: value for key, value in self.__dict__.items()\n                if value is not None}\n\n\n    def build(self) -> \"ModelConfig\":\n        \"\"\"\n        Optionally finalize this configuration.\n\n        Perform validations or transformations if necessary, then\n        return the fully configured instance.\n\n        :return: The current ModelConfig instance (fully configured).\n        \"\"\"\n        # Example validation check:\n        #if self.n < 1:\n        #    raise ValueError(\"Parameter 'n' must be >= 1.\")\n        return self\n\n \n",
                "model_config_adapter": "\"\"\"\nford begin_TODO\n- Consider renaming the class \"ConfigAdapter\" to \"ModelConfigAdapter\" for clarity.\n- In the adapt() method, consider renaming the parameter \"model\" to \"model_instance\" to avoid potential confusion.\n- Update the adapt() docstring: change \"model_type\" to \"model\" since a Model instance is expected.\n- Replace print statements with logging for production use.\nend_todo\n\nImplements a ModelConfigAdapter that specializes a generic Config\npreset based on the provided model_type. This adapter can remove or adapt\nparameters that don\u2019t apply to a given model.\n\"\"\"\n\nimport copy\nfrom models import Config, Model\n\nclass ConfigAdapter:\n    # Define restrictions or adjustments based on model type.\n    # For example, which parameters are not applicable for a model.\n    _restricted_params = {\n        # For model type \"o3-mini\", remove these config keys.\n        \"gpt-4o-mini\": [\"reasoning_effort\"],\n        # For model type \"gpt-4o-mini\", remove or adjust these keys.\n        \"gpt-4o\": [\"reasoning_effort\"],\n        \"gpt-4.5\": [\"reasoning_effort\"],\n        \"o3-mini\": [\"frequency_penalty\", \"top_p\", \"presence_penalty\", \"temperature\"],\n        \"o1\": [\"frequency_penalty\", \"top_p\", \"presence_penalty\", \"temperature\"]\n    }\n\n    @staticmethod\n    def adapt(config: Config, model: Model) -> Config:\n        \"\"\"\n        Adapts the given configuration for the specified model.\n\n        This method examines the model's model_type and removes, nullifies,\n        or adjusts parameters in the configuration that are not applicable\n        for that specific model type.\n\n        Args:\n            config (Config): A Config instance containing the preset parameters.\n            model (Model): A Model instance whose model_type is used to determine\n                           the necessary adaptations.\n\n        Returns:\n            Config: The adapted Config instance with modifications applied\n                  according to the model's restrictions.\n        \"\"\"\n        # Retrieve current parameters as a dict, if needed.\n        sp_config = copy.copy(config)\n        params = sp_config.get_params()\n\n        # Get the list of keys to restrict for this model, if any.\n        model_type = model.model_type\n        print(model_type)\n        keys_to_remove = ConfigAdapter._restricted_params.get(model_type, [])\n\n        # Remove or adjust each restricted parameter.\n        for key in keys_to_remove:\n            if key in params:\n                # Option A: Remove the parameter completely.\n                # One way is to set the attribute to None.\n                setattr(sp_config, key, None)\n                # Alternatively, if you want to remove from the dictionary,\n                # you could delete it from params\u2014but note that\n                # config.get_params() builds the dictionary dynamically.\n                # So setting the underlying attribute to None is sufficient.\n        # Further adaptation rules can go here:\n        # For instance, if a parameter\u2019s value is out of range for a model,\n        # you might adjust it:\n        if (model_type == \"gpt-4o-mini\") or (model_type == \"gpt-4o\"):\n            sp_config.set_reasoning_effort(None)\n\n        # At this point, you can either return the modified config\n        # or even build a fresh dict if desired.\n        return sp_config\n",
                "model_config_director": "\"\"\"\nford begin_TODO\n- Consider renaming the imported class \"Config\" to \"ModelConfig\" for better clarity.\n- Consider renaming \"ConfigDirector\" to \"ModelConfigDirector\" to reflect its role.\n- Update the return type annotations and docstrings in setter methods accordingly if renaming.\n- Ensure consistency in naming conventions across modules (e.g., \"Config\" vs \"ModelConfig\").\nend_todo\n\nThis module defines a Director class for constructing preset ModelConfig\ninstances. The Director orchestrates the configuration steps needed for\nstandard or specialized configurations.\n\"\"\"\n\nfrom typing import Any\nfrom models import Config\n\nclass ConfigDirector:\n    \"\"\"\n    The Director in the Builder pattern. Responsible for creating\n    preset configurations for Config.\n    \"\"\"\n\n    @staticmethod\n    def default_config() -> Config:\n        \"\"\"\n        Build and return a ModelConfig instance with default values.\n\n        :return: A ModelConfig instance using its own defaults.\n        \"\"\"\n        # If you want to override any defaults, chain the setter methods here.\n        # For now, this simply returns a freshly built config as-is.\n        return Config().build()\n\n\n    @staticmethod\n    def reliable_config() -> Config:\n        \"\"\"\n        Build and return a ModelConfig instance with reliable settings.\n\n        :return: A ModelConfig instance with preset parameters for reliable operation.\n        \"\"\"\n        # If you want to override any defaults, chain the setter methods here.\n        # For now, this simply returns a freshly built config as-is.\n        return Config().set_max_completion_tokens(16384) \\\n                       .set_reasoning_effort(\"high\") \\\n                       .set_max_completion_tokens(16384) \\\n                       .build() \n\n    @staticmethod\n    def extensive_config() -> Config:\n        \"\"\"\n        Build and return a ModelConfig instance with extensive settings.\n\n        :return: A ModelConfig instance with additional parameters for extensive use cases.\n        \"\"\"\n        # If you want to override any defaults, chain the setter methods here.\n        # For now, this simply returns a freshly built config as-is.\n        return Config().set_max_completion_tokens(16384) \\\n                       .set_reasoning_effort(\"high\") \\\n                       .set_presence_penalty(1.35) \\\n                       .set_max_completion_tokens(16384) \\\n                       .build() \n\n\n    @staticmethod\n    def creative_config() -> Config:\n        \"\"\"\n        Build and return a ModelConfig instance with creative settings.\n\n        :return: A ModelConfig instance configured for creative applications.\n        \"\"\"\n        # If you want to override any defaults, chain the setter methods here.\n        # For now, this simply returns a freshly built config as-is.\n        return Config().set_max_completion_tokens(16384) \\\n                       .set_reasoning_effort(\"high\") \\\n                       .set_presence_penalty(1.7) \\\n                       .set_max_completion_tokens(16384) \\\n                       .set_temperature(1.4) \\\n                       .build()\n",
                "model_director": "\"\"\"\nford begin_TODO\n- Rename class \"Director\" to \"ModelDirector\" for greater clarity in the context of model configuration.\n- Update docstrings for each method to clearly differentiate the configurations provided.\n- Verify consistency in terminology between \"developer\" and \"administrator\" in the documentation.\nend_todo\n\nModule: models.model_director\nDescription: This module defines a director class responsible for orchestrating the creation\nof different Model configurations using the builder pattern. The director leverages the Model\nclass to provide preset configurations for default use, Python programming expertise, and creative writing.\n\"\"\"\n\nfrom models import Model\n\n\nclass Director:\n    \"\"\"\n    Director class in the Builder pattern responsible for orchestrating\n    the configuration of Model instances.\n\n    This class provides static methods that build Model instances with preset configurations.\n    \"\"\"\n\n    @staticmethod\n    def default_model() -> Model:\n        \"\"\"\n        Build and return a default-configured Model instance.\n\n        The default configuration sets the model type to \"gpt-4o-mini\" and assigns a generic\n        developer instruction that prompts the model to provide concise answers.\n\n        Returns:\n            Model: A fully built Model instance with default settings.\n        \"\"\"\n        return (\n            Model()\n            .set_model_type(\"gpt-4o-mini\")\n            .set_developer_instruction(\"You are a generic assistant that tries to answer as concise as possible\")\n            .build()\n        )\n\n    @staticmethod\n    def python_programmer() -> Model:\n        \"\"\"\n        Build and return a Model instance configured for a Python programming expert.\n\n        This configuration sets the model type to \"o3-mini\" and provides a detailed developer instruction\n        for code review tasks. The instruction emphasizes a direct language, brevity, and a focus on\n        consistency, documentation, PEP compliance, and common Python pitfalls.\n\n        Returns:\n            Model: A fully built Model instance configured for Python programming review.\n        \"\"\"\n        return (\n            Model()\n            .set_model_type(\"o3-mini\")\n            .set_developer_instruction(\n                \"You are a python programming expert that is overviewing the software of the user. \"\n                \"You should use a direct language with a short amount of words and provide information \"\n                \"about lack of consistency, lack of documentation, problems with PEP compliance, and check \"\n                \"for common pitfalls occurring when programming with Python. Readability and maintainability \"\n                \"should be priorities.\"\n            )\n            .build()\n        )\n\n    @staticmethod\n    def writer() -> Model:\n        \"\"\"\n        Build and return a Model instance configured for creative writing tasks.\n\n        This configuration sets the model type to \"gpt-4o\" and assigns a creative writer instruction.\n\n        Returns:\n            Model: A fully built Model instance configured for creative writing.\n        \"\"\"\n        return (\n            Model()\n            .set_model_type(\"gpt-4o\")\n            .set_developer_instruction(\"You are a creative writer\")\n            .build()\n        )\n",
                "__init__": "\"\"\"\nford begin_TODO\n- Consider renaming \"Config\" to \"ModelConfig\" for better clarity and consistency.\n- Consider renaming \"ConfigAdapter\" to \"ModelConfigAdapter\" to clearly indicate its purpose.\n- Consider renaming \"ConfigDirector\" to \"ModelConfigDirector\" for consistency with naming conventions.\n- Consider renaming \"Director\" to \"ModelDirector\" to reflect its role in model construction.\nend_todo\n\nModule: models\nVersion: 1.0.0\n\nThis package provides the core components for model configuration and construction.\nIt aggregates the following modules:\n    - model_config (Config): A builder-style class for configuring chat completion\n      parameters.\n    - model (Model): Defines the model configuration and behavior.\n    - model_config_adapter (ConfigAdapter): Specializes a generic configuration to\n      the specific requirements of a given model type.\n    - model_config_director (ConfigDirector): Provides preset configurations for models.\n    - model_director (Director): Orchestrates the creation of different types of models\n      using the builder pattern.\n\"\"\"\n\nfrom .model_config import Config\nfrom .model import Model\nfrom .model_config_adapter import ConfigAdapter\nfrom .model_config_director import ConfigDirector\nfrom .model_director import Director\n"
            }
        },
        "tools": {
            "files": {
                "model_tool": "\nfrom tools import function_to_schema\n\n\nclass ModelTool:\n\n    def __init__(self) -> None:\n        self.tool_name: str = False\n        self.tool_schema: dict = None\n        self.tool_function: object = None\n        self.raw_last_answer = None\n        self.last_answer = None\n        \n    def set_function(self, external_function: object)-> None:\n        self.tool_schema = function_to_schema(external_function)\n        self.tool_function = external_function\n        self.tool_name = external_function.__name__\n        return self\n    \n    def call_function(self, arguments: dict):\n        self.raw_last_answer = self.tool_function(**arguments)\n        self.last_answer=\"You got the answer:\"+str(self.raw_last_answer)+\"now report it to the user\"\n        return self.raw_last_answer\n    \n        \n        \n    \n\n    \n \n ",
                "model_tool_list": "\nfrom tools import ModelTool\n\nclass ModelToolList:\n    \"\"\"\n    A container for ModelTool objects that prevents duplicate names,\n    allows iteration, and supports name-based lookups.\n    \"\"\"\n    def __init__(self):\n        # Use a dict keyed by tool_name to prevent duplicates and allow quick lookup\n        self._tools = {}\n\n    def add_tool(self, tool: ModelTool) -> None:\n        \"\"\"\n        Add a ModelTool to the list, ensuring no two tools have the same name.\n        Raises a ValueError if the name already exists.\n        \"\"\"\n        if tool.tool_name in self._tools:\n            raise ValueError(f\"Tool with name '{tool.tool_name}' already exists.\")\n        self._tools[tool.tool_name] = tool\n\n    def get_tool_by_name(self, name: str) -> ModelTool:\n        \"\"\"\n        Retrieve a ModelTool by its tool_name.\n        Returns None if no such tool is found.\n        \"\"\"\n        return self._tools.get(name)\n\n    def get_all_functions(self) -> list:\n        \"\"\"\n        Return a list of the `tool_function` objects from all ModelTools.\n        \"\"\"\n        return [tool.tool_function for tool in self._tools.values()]\n\n    def get_all_schemas(self) -> list:\n        \"\"\"\n        Return a list of the `tool_schema` dictionaries from all ModelTools.\n        \"\"\"\n        return [tool.tool_schema for tool in self._tools.values()]\n\n    def __iter__(self):\n        \"\"\"\n        Make the container iterable, returning each ModelTool object in the collection.\n        \"\"\"\n        return iter(self._tools.values())\n\n    def __len__(self):\n        \"\"\"\n        Return the number of ModelTools in the container.\n        \"\"\"\n        return len(self._tools)\n\n\n    \n \n ",
                "schema_helpers": "import inspect\nimport json\nimport typing\nfrom typing import Any, Callable, Dict, Optional, get_type_hints\n\ndef function_to_schema(func: Callable) -> Dict[str, Any]:\n    \"\"\"\n    Convert a Python function (with type hints and docstring) into\n    a JSON-serializable schema object, similar to the examples shown.\n\n    :param func: The function to inspect.\n    :return: A dictionary containing the schema.\n    \"\"\"\n    # 1) Basic function metadata\n    func_name = func.__name__\n    docstring = inspect.getdoc(func) or \"\"  # Full docstring text\n    # You can parse docstring to match your style; we keep it simple here\n\n    # 2) Get signature & type hints\n    sig = inspect.signature(func)\n    type_hints = get_type_hints(func)\n\n    # 3) Prepare the top-level schema structure\n    schema = {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": func_name,\n            \"description\": docstring,\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {},\n                \"required\": [],\n                \"additionalProperties\": False\n            },\n            # We\u2019ll store the return type as well, though your examples often omit it\n            \"strict\": True,  # or set dynamically if needed\n        }\n    }\n\n    # 4) Build the parameters portion from signature\n    for param_name, param in sig.parameters.items():\n        # For each parameter, figure out:\n        #   - type\n        #   - optional or required\n        #   - doc (extracted from docstring or fallback)\n        #   - default value (if any)\n        #   - etc.\n\n        # Type hint\n        annotated_type = type_hints.get(param_name, Any)\n        # Convert Python type to JSON schema style string\n        json_type = python_type_to_json_type(annotated_type)\n\n        # Check if this param is required\n        has_default = (param.default != inspect.Parameter.empty)\n        if not has_default:\n            schema[\"function\"][\"parameters\"][\"required\"].append(param_name)\n\n        # Attempt to find a short description for this param from docstring\n        # (In practice, you'd parse the docstring carefully; we'll skip that.)\n        param_description = f\"Parameter: {param_name}\"\n\n        # Construct property definition\n        schema[\"function\"][\"parameters\"][\"properties\"][param_name] = {\n            \"type\": json_type,\n            \"description\": param_description\n        }\n\n        # If you want to reflect param.default in the schema, you can do so here\n        # but your examples didn't do that explicitly.\n\n    return schema\n\ndef python_type_to_json_type(py_type: Any) -> Any:\n    \"\"\"\n    Helper to map Python type hints to JSON Schema types.\n    This is simplistic. For more robust handling, check for\n    generics like Union, Optional, etc.\n    \"\"\"\n    if py_type == str:\n        return \"string\"\n    elif py_type == int or py_type == float:\n        return \"number\"\n    elif py_type == bool:\n        return \"boolean\"\n    # For Optional[...] or Union[..., None], you might return [\"string\", \"null\"], etc.\n    # For demonstration, keep it simple:\n    return \"string\"\n",
                "__init__": "from .schema_helpers import function_to_schema, python_type_to_json_type \nfrom .model_tool import ModelTool \nfrom .model_tool_list import ModelToolList\n"
            }
        }
    }
}